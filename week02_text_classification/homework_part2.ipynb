{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary prediction, episode II: make it actually work (4 points)\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__. Try __at least 3 options__ from the list below for a passing grade. Write a short report about what you have tried. More ideas = more bonus points. \n",
    "\n",
    "__Please be serious:__ \" plot learning curves in MAE/epoch, compare models based on optimal performance, test one change at a time. You know the drill :)\n",
    "\n",
    "You can use either __pytorch__ or __tensorflow__ or any other framework (e.g. pure __keras__). Feel free to adapt the seminar code for your needs. For tensorflow version, consider `seminar_tf2.ipynb` as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from scipy.stats import beta, shapiro\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244768, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data.drop([\"LocationRaw\", \"SalaryRaw\", \"SourceName\", \"Id\"], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>27500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                        Engineering Systems Analyst   \n",
       "1                            Stress Engineer Glasgow   \n",
       "2                   Modelling and simulation analyst   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription LocationNormalized  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...            Dorking   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...            Glasgow   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...          Hampshire   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...             Surrey   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...             Surrey   \n",
       "\n",
       "  ContractType ContractTime                       Company          Category  \\\n",
       "0          NaN    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1          NaN    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2          NaN    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3          NaN    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4          NaN    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "   SalaryNormalized  \n",
       "0             25000  \n",
       "1             30000  \n",
       "2             30000  \n",
       "3             27500  \n",
       "4             25000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title                      1\n",
       "FullDescription            0\n",
       "LocationNormalized         0\n",
       "ContractType          179326\n",
       "ContractTime           63905\n",
       "Company                32430\n",
       "Category                   0\n",
       "SalaryNormalized           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\n",
    "    \"Category\",\n",
    "    \"Company\",\n",
    "    \"LocationNormalized\",\n",
    "    \"ContractType\",\n",
    "    \"ContractTime\",\n",
    "]\n",
    "target_column = \"SalaryNormalized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just fill all the `NA` values with `NaN` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195814, 8), (48954, 8))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=SEED)\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_train.shape, data_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195814 entries, 0 to 195813\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Title               195814 non-null  object\n",
      " 1   FullDescription     195814 non-null  object\n",
      " 2   LocationNormalized  195814 non-null  object\n",
      " 3   ContractType        195814 non-null  object\n",
      " 4   ContractTime        195814 non-null  object\n",
      " 5   Company             195814 non-null  object\n",
      " 6   Category            195814 non-null  object\n",
      " 7   SalaryNormalized    195814 non-null  int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log1pSalary = np.log1p(data_train[\"SalaryNormalized\"]).astype(\"float32\")\n",
    "\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[\"SalaryNormalized\"], bins=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(log1pSalary, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lognormal distribution is suitable for describing salaries, price of securities, urban population, number of comments on articles on the internet, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryblo\\Documents\\projects\\yandex_nlp_course\\venv\\lib\\site-packages\\scipy\\stats\\_morestats.py:1761: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.9969059824943542, pvalue=2.802596928649634e-45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(log1pSalary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Category' column contains 29 unique values\n",
      "'Company' column contains 18857 unique values\n",
      "'LocationNormalized' column contains 2543 unique values\n",
      "'ContractType' column contains 3 unique values\n",
      "'ContractTime' column contains 3 unique values\n"
     ]
    }
   ],
   "source": [
    "for cat_column in categorical_columns:\n",
    "    print(f\"'{cat_column}' column contains {data_train[cat_column].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IT Jobs                             30847\n",
       "Engineering Jobs                    20159\n",
       "Accounting & Finance Jobs           17434\n",
       "Healthcare & Nursing Jobs           16916\n",
       "Sales Jobs                          13849\n",
       "Other/General Jobs                  13567\n",
       "Teaching Jobs                       10137\n",
       "Hospitality & Catering Jobs          9066\n",
       "Trade & Construction Jobs            7088\n",
       "PR, Advertising & Marketing Jobs     7070\n",
       "HR & Recruitment Jobs                6190\n",
       "Admin Jobs                           6112\n",
       "Retail Jobs                          5201\n",
       "Customer Services Jobs               4794\n",
       "Legal Jobs                           3145\n",
       "Manufacturing Jobs                   3027\n",
       "Logistics & Warehouse Jobs           2912\n",
       "Social work Jobs                     2743\n",
       "Consultancy Jobs                     2591\n",
       "Travel Jobs                          2508\n",
       "Scientific & QA Jobs                 2010\n",
       "Charity & Voluntary Jobs             1867\n",
       "Energy, Oil & Gas Jobs               1821\n",
       "Creative & Design Jobs               1285\n",
       "Maintenance Jobs                     1235\n",
       "Graduate Jobs                        1067\n",
       "Property Jobs                         826\n",
       "Domestic help & Cleaning Jobs         227\n",
       "Part time Jobs                        120\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Company column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                                      25884\n",
       "UKStaffsearch                                             4015\n",
       "CVbrowser                                                 2387\n",
       "London4Jobs                                               1859\n",
       "Hays                                                      1424\n",
       "                                                         ...  \n",
       "Oxford University Hospitals NHS Trust                        1\n",
       "Aspirations Care                                             1\n",
       "MANPOWER UK LTD                                              1\n",
       "Zolv.com Ltd                                                 1\n",
       "The Facial Surgery Research Foundation   Saving Faces        1\n",
       "Name: Company, Length: 18857, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Company\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LocationNormalized column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UK                   32767\n",
       "London               24483\n",
       "South East London     9411\n",
       "The City              5337\n",
       "Manchester            2859\n",
       "                     ...  \n",
       "Kilbarchan               1\n",
       "Brompton                 1\n",
       "Kennoway                 1\n",
       "Kirkby Stephen           1\n",
       "Grimethorpe              1\n",
       "Name: LocationNormalized, Length: 2543, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"LocationNormalized\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ContractType column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN          143245\n",
       "full_time     46208\n",
       "part_time      6361\n",
       "Name: ContractType, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"ContractType\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ContractTime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permanent    121042\n",
       "NaN           51332\n",
       "contract      23440\n",
       "Name: ContractTime, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"ContractTime\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "# tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "# assert tokenizer.tokenize(\"I've been doing this for a few years!\") == [\n",
    "#     \"I've\",\n",
    "#     \"been\",\n",
    "#     \"doing\",\n",
    "#     \"this\",\n",
    "#     \"for\",\n",
    "#     \"a\",\n",
    "#     \"few\",\n",
    "#     \"years\",\n",
    "#     \"!\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessDataset:\n",
    "    TEXT_COLS = [\"Title\", \"FullDescription\"]\n",
    "    CAT_COLS = [\n",
    "        \"Category\",\n",
    "        \"Company\",\n",
    "        \"LocationNormalized\",\n",
    "        \"ContractType\",\n",
    "        \"ContractTime\",\n",
    "    ]\n",
    "    TARGET_COL = \"SalaryNormalized\"\n",
    "    NEW_TARGET_COL = \"Log1pSalary\"\n",
    "    UNK, PAD = \"UNK\", \"PAD\"\n",
    "    SEED = 42\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        min_count=10,\n",
    "        max_len_title=None,\n",
    "        max_len_description=None,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "        self.data_train, self.data_val = self._split_data()\n",
    "\n",
    "        self.data_train = self.preprocess_data(self.data_train, train=True)\n",
    "        self.data_val = self.preprocess_data(self.data_val)\n",
    "\n",
    "        self.categorical_vectorizer = self._preprocess_categorical()\n",
    "        self.vocab = self._build_vocab(min_count)\n",
    "        self.token_to_id = self._build_token_to_id_dict()\n",
    "        self.UNK_IX, self.PAD_IX = map(self.token_to_id.get, [self.UNK, self.PAD])\n",
    "        self.max_len_description = max_len_description\n",
    "        self.max_len_title = max_len_title\n",
    "\n",
    "    def preprocess_data(self, data, train=False):\n",
    "        print(f\"Preprocessing {'Train' if train else 'Val'} data\")\n",
    "        data = self._preprocess_texts(data)\n",
    "        data = self._preprocess_target(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _split_data(self, test_size=0.2):\n",
    "        print(\"Splitting data to train and val sets\")\n",
    "        data_train, data_val = train_test_split(self.data, test_size=test_size, random_state=self.SEED)\n",
    "        data_train.reset_index(drop=True, inplace=True)\n",
    "        data_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return data_train, data_val\n",
    "\n",
    "    def _preprocess_texts(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for text_col in self.TEXT_COLS:\n",
    "            tqdm.pandas(desc=f\"Tokenizing {text_col}\")\n",
    "            tokenized_texts = data[text_col].progress_apply(self._tokenize)\n",
    "            data[text_col] = tokenized_texts.values\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _preprocess_target(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"Preprocessing Target column\")\n",
    "        data[self.NEW_TARGET_COL] = np.log1p(data[self.TARGET_COL]).astype(\"float32\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _preprocess_categorical(self, most_common=1000):\n",
    "        print(\"Preprocessing Categorical columns\")\n",
    "\n",
    "        tqdm.pandas(desc=f\"Preprocessing Company column\")\n",
    "        # we only consider top-1k most frequent companies to minimize memory usage\n",
    "        top_companies, top_counts = zip(*Counter(self.data_train[\"Company\"]).most_common(most_common))\n",
    "        recognized_companies = set(top_companies)\n",
    "        self.data_train[\"Company\"] = self.data_train[\"Company\"].progress_apply(\n",
    "            lambda comp: comp if comp in recognized_companies else \"Other\"\n",
    "        )\n",
    "\n",
    "        tqdm.pandas(desc=f\"Vectorizing categorical columns\")\n",
    "        categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "        categorical_vectorizer.fit(self.data_train[self.CAT_COLS].progress_apply(dict, axis=1))\n",
    "\n",
    "        return categorical_vectorizer\n",
    "\n",
    "    def _build_vocab(self, min_count):\n",
    "        token_counts = Counter()\n",
    "\n",
    "        # Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
    "        for str_ in tqdm(self.data_train[text_columns].values.flatten(), desc=\"Building vocab\"):\n",
    "            token_counts.update(str(str_).split())\n",
    "\n",
    "        print(\"Total unique tokens :\", len(token_counts))\n",
    "        print(\"\\n\".join(map(str, token_counts.most_common(n=5))))\n",
    "        print(\"...\")\n",
    "        print(\"\\n\".join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "        # tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "        tokens = sorted(t for t, c in token_counts.items() if c >= min_count)\n",
    "\n",
    "        # Add a special tokens for unknown and empty words\n",
    "        tokens = [self.UNK, self.PAD] + tokens\n",
    "\n",
    "        print(\"Vocabulary size:\", len(tokens))\n",
    "        assert type(tokens) == list\n",
    "        assert len(tokens) in range(30000, 35000)\n",
    "        assert \"me\" in tokens\n",
    "        assert self.UNK in tokens\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build_token_to_id_dict(self):\n",
    "        token_to_id = {token: i for i, token in enumerate(self.vocab)}\n",
    "\n",
    "        assert isinstance(token_to_id, dict)\n",
    "        assert len(token_to_id) == len(self.vocab)\n",
    "        for token in self.vocab:\n",
    "            assert self.vocab[token_to_id[token]] == token\n",
    "\n",
    "        return token_to_id\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return \" \".join(self.tokenizer.tokenize(str(text).lower()))\n",
    "\n",
    "    def _as_matrix(self, sequences, max_len=None):\n",
    "        \"\"\"Convert a list of tokens into a matrix with padding\"\"\"\n",
    "\n",
    "        if isinstance(sequences[0], str):\n",
    "            sequences = list(map(str.split, sequences))\n",
    "\n",
    "        max_len = min(max(map(len, sequences)), max_len or float(\"inf\"))\n",
    "\n",
    "        matrix = np.full((len(sequences), max_len), np.int32(self.PAD_IX))\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            row_ix = [self.token_to_id.get(word, self.UNK_IX) for word in seq[:max_len]]\n",
    "            matrix[i, : len(row_ix)] = row_ix\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def _encode_categorical(self, data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data to train and val sets\n",
      "Preprocessing Train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deb53035e7c41e2958f21c0879804d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Title:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cbb3f7b882418ea21c45589e3f224c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing FullDescription:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Target column\n",
      "Preprocessing Val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296c074b5ebd4d98bd9655f5e8741282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Title:   0%|          | 0/48954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09796498eebc4563a74cabc5a7f9fa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing FullDescription:   0%|          | 0/48954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Target column\n",
      "Preprocessing Categorical columns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95415e094df423d96af6eac34dbc262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Company column:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a752c891f9a34d509b6e21ad9b3a4300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing categorical columns:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a595bacd4fc4c56ac860d14e08275f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vocab:   0%|          | 0/391628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 201108\n",
      "('and', 2123081)\n",
      "(',', 1871868)\n",
      "('.', 1758719)\n",
      "('*', 1674219)\n",
      "('the', 1659371)\n",
      "...\n",
      "('pate', 1)\n",
      "('nurseportadown', 1)\n",
      "('www.salestarget.co.uk/jobseeking/salesexecutivesx2northwestlondonesp***_job***', 1)\n",
      "Vocabulary size: 31171\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreprocessDataset(data, tokenizer, max_len_title=10, max_len_description=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Dataloaders & Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pass training data with parameter `train=True` with which we will fit our vectorizers and so on and test data with `train=False` with which we will transform our test dataset  \n",
    "Create one class for dataset preprocessing and another class for using preprocessed dataset for dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add target transform method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyWordDropout:\n",
    "    def __init__(self, replace_with, pad_ix, word_dropout=0.0):\n",
    "        self.keep_prop = 1.0 - word_dropout\n",
    "        self.replace_with = replace_with\n",
    "        self.pad_ix = pad_ix\n",
    "\n",
    "    def apply_word_dropout(self, matrix):\n",
    "        dropout_mask = np.random.choice(2, np.shape(matrix), p=[self.keep_prop, 1 - self.keep_prop])\n",
    "        dropout_mask &= matrix != self.pad_ix\n",
    "\n",
    "        return np.choose(dropout_mask, [matrix, np.full_like(matrix, self.replace_with)])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return self.apply_word_dropout(sample)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, device=DEVICE):\n",
    "        sample_tensors = dict()\n",
    "        for key, arr in sample.items():\n",
    "            if key in [\"FullDescription\", \"Title\"]:\n",
    "                sample_tensors[key] = torch.tensor(arr, dtype=torch.long, device=device)\n",
    "            else:\n",
    "                sample_tensors[key] = torch.tensor(arr, device=device)\n",
    "        return sample_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VacancyDataset:\n",
    "    def __init__(self, preprocessor, train=True, transform=None, word_dropout=0):\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "        self.data = self.preprocessor.data_train if train else preprocessor.data_val\n",
    "\n",
    "        self.transform = transform\n",
    "        self.word_dropout = ApplyWordDropout(\n",
    "            replace_with=self.preprocessor.UNK_IX,\n",
    "            pad_ix=self.preprocessor.PAD_IX,\n",
    "            word_dropout=word_dropout,\n",
    "        )\n",
    "\n",
    "        self.title = self.preprocessor._as_matrix(self.data[\"Title\"].values, self.preprocessor.max_len_title)\n",
    "        self.description = self.preprocessor._as_matrix(\n",
    "            self.data[\"FullDescription\"].values,\n",
    "            self.preprocessor.max_len_description,\n",
    "        )\n",
    "\n",
    "        if word_dropout != 0:\n",
    "            self.description = self.word_dropout(self.description)\n",
    "\n",
    "        self.categorical = self.preprocessor.categorical_vectorizer.transform(\n",
    "            self.data[self.preprocessor.CAT_COLS].apply(dict, axis=1)\n",
    "        )\n",
    "\n",
    "        self.target = self.data[self.preprocessor.NEW_TARGET_COL]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        sample[\"title_text\"] = self.title[idx]\n",
    "        sample[\"description_text\"] = self.description[idx]\n",
    "        sample[\"categorical\"] = self.categorical[idx]\n",
    "        sample[\"target\"] = self.target[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = VacancyDataset(preprocessor, train=True, transform=ToTensor())\n",
    "test_data = VacancyDataset(preprocessor, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_text': tensor([24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'description_text': tensor([24816, 26825, 30259, 29588,   819, 24816, 26825, 30259, 14759, 29588,\n",
       "         11235,   819, 23396, 24881, 13932,     0,    14, 25722,    14, 11235,\n",
       "         23397, 30147, 15941,  8200,    14, 20844,  8286,    14,  1106,  3973,\n",
       "         14235,  1927,     0, 21736,    15, 27600, 13699,  4640, 30041, 12710,\n",
       "          9263,  1883, 19240,    12,    12,    12,    16,    12,    12,    12,\n",
       "         30147,  2591, 15954,   819, 31005, 10309, 30158, 27696, 10848,    14,\n",
       "         21486,  2591,   819, 24816, 16073,    15, 27600, 24816, 26825, 30259,\n",
       "         30041, 12710, 27916,  3220,   898, 27916,  6152,  1927, 13886,  4739,\n",
       "         20988,    14,  2389, 29809,  2389,  2328, 14063,  7402, 19941,  1927,\n",
       "          9527, 23397, 27916, 27125,  1883,  1140, 23897, 13932,  1669,  2467],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'categorical': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'),\n",
       " 'target': tensor(9.7115, device='cuda:0')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_text': tensor([[12821,  1927, 25625,  4739,  2177,  2511,     1,     1,     1,     1],\n",
       "         [15268,  1895, 13659,  7961,  9601, 21339, 16901, 28588,     1,     1],\n",
       "         [20561, 16910,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [27290, 15904, 31140, 14830, 22774,    14,  3917,    14,  3478,    12],\n",
       "         [24816, 24233,  1366, 30908,    12,    12,    12,     7, 11638,     5],\n",
       "         [21858, 16910,  5528,    12,    12,    12, 15301, 10158,  3370,    16],\n",
       "         [22774,  6487, 24816, 10910,     7, 20074,     9,     1,     1,     1],\n",
       "         [27465,  7865, 19213, 25063, 14783,     1,     1,     1,     1,     1],\n",
       "         [18330, 17087, 16910,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [23524, 24233,  2529,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [13402,  1276,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 5326,  7579, 20288, 23328,  5333,     1,     1,     1,     1,     1],\n",
       "         [24816, 20984, 16910,  5086, 16464,     1,     1,     1,     1,     1],\n",
       "         [24816,  2712,     5, 14626, 16910,     1,     1,     1,     1,     1],\n",
       "         [24233,     5, 16069, 18665,  9967,  1470, 19075, 29883, 16464,     1],\n",
       "         [24233, 15974,  5646,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24233,  1276,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 4392,  5208,  1905,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 4557,  5093, 19657,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [15994, 24650,     7, 10594,  7874,     9, 27569,    12,    12,    12],\n",
       "         [26270, 10494,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [27348, 26825,  9601, 11929,     1,     1,     1,     1,     1,     1],\n",
       "         [ 7961,  1927,  8058,     7,  3777,  6261, 26522,     9,     1,     1],\n",
       "         [22068, 17418, 18060,    16, 23360, 20846,     1,     1,     1,     1],\n",
       "         [ 5779,     0,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [21778, 10209,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [28167, 16910,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [11851, 16910,    14,  9695,  6177,     1,     1,     1,     1,     1],\n",
       "         [ 7986, 26825,  9601,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24816,  1016, 16910,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 7924, 16910,  4739, 13223,     7, 23657,     9,     1,     1,     1],\n",
       "         [23071, 16910,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [22275, 14321,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 2177,  2511,    16, 16901,    16,  7260, 24881,    16,  1262,     1],\n",
       "         [29719,  8052,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [22985, 11851, 19213,  3606,     1,     1,     1,     1,     1,     1],\n",
       "         [22293, 26882,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [23949, 15904,     7, 17198,  6878,     9,     1,     1,     1,     1],\n",
       "         [21858, 16910,  2765,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24816, 16901, 26882,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 3991, 16910,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24834,  1927,  6502, 24493,     1,     1,     1,     1,     1,     1],\n",
       "         [   12,    12,    12, 12182,    16, 12182,  4553, 30147, 24233, 29040],\n",
       "         [ 5326,  7579, 20290,  1569,  4927, 19651,     1,     1,     1,     1],\n",
       "         [21858, 16910,  4721, 20415,     1,     1,     1,     1,     1,     1],\n",
       "         [11451,  9979,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 2529, 16910,  4890,  6866,     1,     1,     1,     1,     1,     1],\n",
       "         [28163, 25687,     7, 27465,     9, 15110, 13932, 24515,     1,     1],\n",
       "         [20825, 24233,    16, 21902,    16,  7260, 24881,     1,     1,     1],\n",
       "         [16394,  4392, 16910, 20566,     1,     1,     1,     1,     1,     1],\n",
       "         [21372,  5559,  1370,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [19654,  6682,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24488, 27281, 19976, 24482, 29883, 16464,     1,     1,     1,     1],\n",
       "         [19347, 12821, 19213,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [18861, 22681,    16, 21202,     1,     1,     1,     1,     1,     1],\n",
       "         [ 1016, 16910,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [10209, 25786,  5326,    12,    12,    12, 14496, 13338, 12331,   936],\n",
       "         [11745, 27369,   794, 17497, 31143, 24165, 18317, 21776,     1,     1],\n",
       "         [21657, 26822, 27284,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [17009,  9601,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 7260,  6512, 19428,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 5509, 18722,  6487,  9601,  5003,  4986,  5136,     1,     1,     1],\n",
       "         [27778,  8052,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [22275, 14321, 28588,     1,     1,     1,     1,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'description_text': tensor([[24683,   819, 28163,  ...,  1927, 27916,  9665],\n",
       "         [19920,  5663,    14,  ..., 28702, 19403,  1896],\n",
       "         [29700,  2273, 21043,  ..., 23065, 10141, 26694],\n",
       "         ...,\n",
       "         [ 5509, 18722,  6487,  ...,    12,    12, 29033],\n",
       "         [27778,  8052, 19075,  ...,  4177,  6199, 27125],\n",
       "         [ 1883, 13830, 21236,  ...,     1,     1,     1]], device='cuda:0',\n",
       "        dtype=torch.int32),\n",
       " 'categorical': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'target': tensor([ 9.8862, 10.3890, 10.8198, 10.3090,  9.8522, 10.9151, 10.0213,  9.9580,\n",
       "         10.9151,  9.3847,  9.7700,  9.6159, 11.2253, 11.2898,  9.6159,  9.3519,\n",
       "          9.7112, 10.7144,  9.4336,  9.5469,  9.7574, 10.2577, 10.2922, 11.1199,\n",
       "         10.2751,  9.9523,  9.9988, 10.2220, 10.4223, 10.3890, 10.1065, 10.7144,\n",
       "          9.8114,  9.8522, 10.7685, 10.4993, 10.4631,  9.6804, 10.2922, 10.8198,\n",
       "         10.4631, 10.3090, 10.2751,  9.9282, 11.0429, 10.6573,  9.7410, 10.0754,\n",
       "          9.6159, 10.2400,  9.6159, 10.3090, 10.4193, 10.4089,  9.8061,  9.9035,\n",
       "         11.0021, 10.3890, 10.3594, 10.3578,  9.6159, 10.6573, 10.9151, 10.6282],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Base Architecture\n",
    "\n",
    "Our basic model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly doesn't fit into __Sequential__ interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens, pad_ix, hid_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> relu -> global_max\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.emb = nn.Embedding(n_tokens, 32, padding_idx=pad_ix)\n",
    "        self.conv1 = nn.Conv1d(32, hid_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        x = self.emb(batch)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        c = self.conv1(x)\n",
    "        p = self.pool1(self.relu(c))\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "title_encoder = BaseTitleEncoder(n_tokens=len(preprocessor.vocab), pad_ix=preprocessor.PAD_IX, hid_size=64)\n",
    "title_encoder.to(DEVICE)\n",
    "\n",
    "dummy_x = Variable(batch[\"title_text\"])\n",
    "dummy_v = title_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Embedding layer investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_text': tensor([[24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'description_text': tensor([[24816, 26825, 30259, 29588,   819, 24816, 26825, 30259, 14759, 29588,\n",
       "          11235,   819, 23396, 24881, 13932,     0,    14, 25722,    14, 11235,\n",
       "          23397, 30147, 15941,  8200,    14, 20844,  8286,    14,  1106,  3973,\n",
       "          14235,  1927,     0, 21736,    15, 27600, 13699,  4640, 30041, 12710,\n",
       "           9263,  1883, 19240,    12,    12,    12,    16,    12,    12,    12,\n",
       "          30147,  2591, 15954,   819, 31005, 10309, 30158, 27696, 10848,    14,\n",
       "          21486,  2591,   819, 24816, 16073,    15, 27600, 24816, 26825, 30259,\n",
       "          30041, 12710, 27916,  3220,   898, 27916,  6152,  1927, 13886,  4739,\n",
       "          20988,    14,  2389, 29809,  2389,  2328, 14063,  7402, 19941,  1927,\n",
       "           9527, 23397, 27916, 27125,  1883,  1140, 23897, 13932,  1669,  2467],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591, 22873,   794,    12,\n",
       "             12,    12,  3131,  5353, 10158, 23174,  1927,  3370, 19920,  5663,\n",
       "          14759, 24683,   819, 27144,     0, 17204,    16, 26233, 17979, 27916,\n",
       "          20275, 13932, 27600,  8058, 19403,  7301,  9139, 17982, 11235,  2126,\n",
       "          13932, 27600, 20771,    14,  1513,  1927,  5335, 14078,    15, 27600,\n",
       "          21236, 30041,  9670, 16859, 25235,  6588, 27916, 24941, 21872, 13932,\n",
       "          25302, 17981,  1927, 13013,  6532, 24537,    15, 27600, 21657, 23337,\n",
       "           2273,   819, 26511,  2927, 13932, 26237, 19745, 17208,    14,   819,\n",
       "           7977, 27916, 30246, 13932,   819, 20771,    16,  3568, 17981,  9748,\n",
       "             14,  1178,  1927,   819, 12115, 27290, 10001,    15, 15099, 22148]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'categorical': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'target': tensor([ 9.7115, 10.4631], device='cuda:0')}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = training_data[:2]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([[[-0.1321,  0.1434, -0.1215, -0.9798, -1.3410, -1.2773,  0.1071,\n",
       "            0.4405, -1.8925, -0.1298],\n",
       "          [-1.2876,  0.7906, -0.6918,  0.6874, -1.4885,  1.3839, -0.4336,\n",
       "            0.8877, -0.7404,  1.2789],\n",
       "          [ 0.1730,  0.9727,  1.8057, -0.1654, -2.2603,  1.8031,  0.4333,\n",
       "            0.1338, -0.3940, -0.0744],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.9537, -0.7064, -0.3966, -0.4719,  0.1437, -0.8028,  0.5573,\n",
       "            0.3370, -0.5116,  0.1256],\n",
       "          [ 0.2215, -0.3094, -1.0025, -1.2162,  0.0299,  1.1782, -0.9082,\n",
       "            0.0493, -0.1553, -0.3798],\n",
       "          [-0.2373,  0.1397, -1.0350,  0.1770,  0.1714, -0.1479, -1.1166,\n",
       "            0.8411,  0.1842, -0.4419],\n",
       "          [ 0.1721, -0.8382, -0.4159,  0.7722,  0.7513, -0.2683,  0.8043,\n",
       "           -0.3489, -1.0702, -0.1528],\n",
       "          [-0.3911,  0.7799, -0.9496, -0.8051, -0.4030, -1.6325, -1.1281,\n",
       "            0.3458,  0.4299, -1.0335],\n",
       "          [-0.2522,  1.0826, -1.0586,  0.4104,  0.0901, -0.3929,  0.1629,\n",
       "           -1.4430, -0.1849,  0.4416],\n",
       "          [-0.7896,  0.6021, -0.5424, -0.0788,  0.0433, -0.7192,  0.8872,\n",
       "            1.1721, -1.2684,  0.1346],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000]]], device='cuda:0',\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(\n",
    "    num_embeddings=len(preprocessor.vocab), embedding_dim=10, padding_idx=preprocessor.PAD_IX, device=DEVICE\n",
    ")\n",
    "title, embedded_title = batch[\"title_text\"], emb(batch[\"title_text\"])\n",
    "title, embedded_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(embedded_title, 1, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens, pad_ix, hid_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for descriptions.\n",
    "        x -> emb -> conv -> relu -> global_max\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.emb = nn.Embedding(n_tokens, 32, padding_idx=pad_ix)\n",
    "        self.conv1 = nn.Conv1d(32, hid_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        x = self.emb(batch)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        c = self.conv1(x)\n",
    "        p = self.pool1(self.relu(c))\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "description_encoder = BaseDescriptionEncoder(n_tokens=len(preprocessor.vocab), pad_ix=preprocessor.PAD_IX, hid_size=64)\n",
    "description_encoder.to(DEVICE)\n",
    "\n",
    "dummy_x = Variable(batch[\"description_text\"])\n",
    "dummy_v = description_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del description_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusion: Title Encoder + Description Encoder + Categorical Encoder + FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSalaryPredictor(nn.Module):\n",
    "    def __init__(self, n_cat_features, n_tokens, pad_ix, hid_size=64):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.title_encoder = BaseTitleEncoder(n_tokens=n_tokens, pad_ix=pad_ix, hid_size=64)\n",
    "        self.desc_encoder = BaseDescriptionEncoder(n_tokens=n_tokens, pad_ix=pad_ix, hid_size=64)\n",
    "\n",
    "        self.cat_dense1 = nn.Linear(n_cat_features, hid_size)\n",
    "        self.cat_dense2 = nn.Linear(hid_size, hid_size)\n",
    "\n",
    "        self.out_dense1 = nn.Linear(192, 100)\n",
    "        self.out_dense2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(batch[\"title_text\"])\n",
    "        desc_h = self.desc_encoder(batch[\"description_text\"])\n",
    "\n",
    "        # apply categorical encoder\n",
    "        cat_h = self.relu(self.cat_dense1(batch[\"categorical\"]))\n",
    "        cat_h = self.cat_dense2(cat_h)\n",
    "\n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "\n",
    "        # ... and stack a few more layers at the top\n",
    "        joint_h = self.out_dense1(joint_h)\n",
    "        joint_h = self.relu(joint_h)\n",
    "        joint_h = self.out_dense2(joint_h)\n",
    "\n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "\n",
    "        return joint_h[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseSalaryPredictor(\n",
    "    n_cat_features=len(preprocessor.categorical_vectorizer.vocabulary_),\n",
    "    n_tokens=len(preprocessor.vocab),\n",
    "    pad_ix=preprocessor.PAD_IX,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dummy_pred = model(batch)\n",
    "dummy_loss = criterion(dummy_pred, batch[\"target\"])\n",
    "assert dummy_pred.shape == torch.Size([64])\n",
    "assert len(torch.unique(dummy_pred)) > 20, \"model returns suspiciously few unique outputs. Check your initialization\"\n",
    "assert dummy_loss.ndim == 0 and 0.0 <= dummy_loss <= 250.0, \"make sure you minimize MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens, pad_ix, hid_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> relu -> global_max\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.drop1 = nn.Dropout()\n",
    "        # self.drop2 = nn.Dropout()\n",
    "        self.emb = nn.Embedding(n_tokens, 32, padding_idx=pad_ix)\n",
    "        self.conv1 = nn.Conv1d(32, hid_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(hid_size, hid_size, kernel_size=3, padding=1)\n",
    "        self.pool2 = GlobalMaxPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        x = self.emb(batch)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        out = torch.transpose(x, 1, 2)\n",
    "\n",
    "        out = self.pool1(self.relu(self.conv1(out)))\n",
    "        out = self.pool2(self.relu(self.conv2(out)))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "title_encoder = TitleEncoder(n_tokens=len(preprocessor.vocab), pad_ix=preprocessor.PAD_IX, hid_size=64)\n",
    "title_encoder.to(DEVICE)\n",
    "\n",
    "dummy_x = Variable(batch[\"title_text\"])\n",
    "dummy_v = title_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_text': tensor([[24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'description_text': tensor([[24816, 26825, 30259, 29588,   819, 24816, 26825, 30259, 14759, 29588,\n",
       "          11235,   819, 23396, 24881, 13932,     0,    14, 25722,    14, 11235,\n",
       "          23397, 30147, 15941,  8200,    14, 20844,  8286,    14,  1106,  3973,\n",
       "          14235,  1927,     0, 21736,    15, 27600, 13699,  4640, 30041, 12710,\n",
       "           9263,  1883, 19240,    12,    12,    12,    16,    12,    12,    12,\n",
       "          30147,  2591, 15954,   819, 31005, 10309, 30158, 27696, 10848,    14,\n",
       "          21486,  2591,   819, 24816, 16073,    15, 27600, 24816, 26825, 30259,\n",
       "          30041, 12710, 27916,  3220,   898, 27916,  6152,  1927, 13886,  4739,\n",
       "          20988,    14,  2389, 29809,  2389,  2328, 14063,  7402, 19941,  1927,\n",
       "           9527, 23397, 27916, 27125,  1883,  1140, 23897, 13932,  1669,  2467],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591, 22873,   794,    12,\n",
       "             12,    12,  3131,  5353, 10158, 23174,  1927,  3370, 19920,  5663,\n",
       "          14759, 24683,   819, 27144,     0, 17204,    16, 26233, 17979, 27916,\n",
       "          20275, 13932, 27600,  8058, 19403,  7301,  9139, 17982, 11235,  2126,\n",
       "          13932, 27600, 20771,    14,  1513,  1927,  5335, 14078,    15, 27600,\n",
       "          21236, 30041,  9670, 16859, 25235,  6588, 27916, 24941, 21872, 13932,\n",
       "          25302, 17981,  1927, 13013,  6532, 24537,    15, 27600, 21657, 23337,\n",
       "           2273,   819, 26511,  2927, 13932, 26237, 19745, 17208,    14,   819,\n",
       "           7977, 27916, 30246, 13932,   819, 20771,    16,  3568, 17981,  9748,\n",
       "             14,  1178,  1927,   819, 12115, 27290, 10001,    15, 15099, 22148]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'categorical': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'target': tensor([ 9.7115, 10.4631], device='cuda:0')}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = training_data[:2]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([[[-2.3162e-01, -5.0554e-01, -4.6597e-01, -1.3484e+00, -8.5686e-01,\n",
       "           -1.1567e+00, -1.8964e+00, -4.5654e-01,  2.4599e-01, -5.1768e-01],\n",
       "          [ 2.3576e+00,  1.5850e+00,  1.8735e+00, -1.3302e+00, -2.8675e-01,\n",
       "            3.1965e-01,  1.8017e+00, -8.9128e-01,  6.9923e-01,  1.7007e-02],\n",
       "          [-4.9324e-01, -8.2238e-01,  1.4447e+00,  2.3752e+00, -1.5268e+00,\n",
       "            2.8876e+00, -1.8820e-01, -3.6543e-01,  1.5590e+00, -1.7589e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 6.2430e-01, -1.3791e+00,  1.8938e+00,  9.1941e-01,  6.2964e-01,\n",
       "            7.9569e-01,  1.4048e+00, -8.5463e-01,  3.1829e-01, -1.2466e+00],\n",
       "          [-2.9016e+00, -1.2871e+00,  7.6809e-01, -5.8296e-01,  1.1435e+00,\n",
       "           -6.8221e-01,  2.6907e+00,  1.8485e-02,  4.0449e-01, -5.5388e-01],\n",
       "          [-5.7506e-01,  2.7264e+00,  1.5035e-01,  5.9355e-01, -2.4925e-01,\n",
       "            8.5615e-02,  2.8432e-01,  5.2554e-01,  1.7456e+00,  6.4535e-01],\n",
       "          [-1.1003e+00, -1.1236e-01,  1.0662e+00,  6.3362e-01, -7.8866e-01,\n",
       "            8.5680e-01,  3.6820e-02,  1.7249e+00, -9.5764e-01, -6.4361e-01],\n",
       "          [-4.7135e-01, -3.6987e-01, -5.9746e-01, -9.8854e-01, -7.8131e-01,\n",
       "            2.2218e+00, -1.3734e+00, -2.0779e+00, -2.3397e-01, -2.5486e-01],\n",
       "          [-4.0707e-01, -1.1270e+00, -4.1813e-01, -2.4859e-01,  9.6409e-01,\n",
       "           -2.7708e-01, -1.4415e-03,  1.3547e+00,  1.0444e-01,  2.7618e-01],\n",
       "          [ 1.8468e+00,  1.6409e+00, -1.3308e+00, -6.0563e-01,  5.3151e-03,\n",
       "           -8.9462e-01, -2.9276e-01, -1.6280e-01, -2.0924e-01,  2.1044e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "        device='cuda:0', grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(\n",
    "    num_embeddings=len(preprocessor.vocab), embedding_dim=10, padding_idx=preprocessor.PAD_IX, device=DEVICE\n",
    ")\n",
    "title, embedded_title = batch[\"title_text\"], emb(batch[\"title_text\"])\n",
    "title, embedded_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(embedded_title, 1, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens, pad_ix, hid_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for descriptions.\n",
    "        x -> emb -> conv -> relu -> global_max\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.emb = nn.Embedding(n_tokens, 32, padding_idx=pad_ix)\n",
    "        self.conv1 = nn.Conv1d(32, hid_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        x = self.emb(batch)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        c = self.conv1(x)\n",
    "        p = self.pool1(self.relu(c))\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "description_encoder = DescriptionEncoder(n_tokens=len(preprocessor.vocab), pad_ix=preprocessor.PAD_IX, hid_size=64)\n",
    "description_encoder.to(DEVICE)\n",
    "\n",
    "dummy_x = Variable(batch[\"description_text\"])\n",
    "dummy_v = description_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del description_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusion: Title Encoder + Description Encoder + Categorical Encoder + FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryPredictor(nn.Module):\n",
    "    def __init__(self, n_cat_features, n_tokens, pad_ix, hid_size=64):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.title_encoder = TitleEncoder(n_tokens=n_tokens, pad_ix=pad_ix, hid_size=64)\n",
    "        self.desc_encoder = DescriptionEncoder(n_tokens=n_tokens, pad_ix=pad_ix, hid_size=64)\n",
    "\n",
    "        self.cat_dense1 = nn.Linear(n_cat_features, hid_size)\n",
    "        self.cat_dense2 = nn.Linear(hid_size, hid_size)\n",
    "\n",
    "        self.out_dense1 = nn.Linear(192, 100)\n",
    "        self.out_dense2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(batch[\"title_text\"])\n",
    "        desc_h = self.desc_encoder(batch[\"description_text\"])\n",
    "\n",
    "        # apply categorical encoder\n",
    "        cat_h = self.relu(self.cat_dense1(batch[\"categorical\"]))\n",
    "        cat_h = self.cat_dense2(cat_h)\n",
    "\n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "\n",
    "        # ... and stack a few more layers at the top\n",
    "        joint_h = self.out_dense1(joint_h)\n",
    "        joint_h = self.relu(joint_h)\n",
    "        joint_h = self.out_dense2(joint_h)\n",
    "\n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "\n",
    "        return joint_h[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SalaryPredictor(\n",
    "    n_cat_features=len(preprocessor.categorical_vectorizer.vocabulary_),\n",
    "    n_tokens=len(preprocessor.vocab),\n",
    "    pad_ix=preprocessor.PAD_IX,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "dummy_pred = model(batch)\n",
    "dummy_loss = criterion(dummy_pred, batch[\"target\"])\n",
    "assert dummy_pred.shape == torch.Size([64])\n",
    "assert len(torch.unique(dummy_pred)) > 20, \"model returns suspiciously few unique outputs. Check your initialization\"\n",
    "assert dummy_loss.ndim == 0 and 0.0 <= dummy_loss <= 250.0, \"make sure you minimize MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    iter_size = 1000\n",
    "    for batch_num, batch in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        y = batch[\"target\"]\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % iter_size == 0:\n",
    "            loss, current = loss.item(), batch_num * len(y)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            y = batch[\"target\"]\n",
    "            pred = model(batch)\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            squared_error += torch.sum(torch.square(pred - y))\n",
    "            abs_error += torch.sum(torch.abs(pred - y))\n",
    "            num_samples += len(pred)\n",
    "\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    print(f\"Test Error: \\n MSE: {mse:>0.2f}, MAE: {mae:>0.2f}, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3366.200195  [    0/195814]\n",
      "loss: 11.910650  [32000/195814]\n",
      "loss: 7.304364  [64000/195814]\n",
      "loss: 4.614039  [96000/195814]\n",
      "loss: 4.778317  [128000/195814]\n",
      "loss: 4.259149  [160000/195814]\n",
      "loss: 3.154397  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.13, MAE: 0.27, Avg loss: 4.077567 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 12.236903  [    0/195814]\n",
      "loss: 5.639375  [32000/195814]\n",
      "loss: 5.917344  [64000/195814]\n",
      "loss: 3.094014  [96000/195814]\n",
      "loss: 2.919345  [128000/195814]\n",
      "loss: 4.017175  [160000/195814]\n",
      "loss: 1.680103  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.11, MAE: 0.25, Avg loss: 3.587349 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.513937  [    0/195814]\n",
      "loss: 2.340133  [32000/195814]\n",
      "loss: 3.851770  [64000/195814]\n",
      "loss: 6.073340  [96000/195814]\n",
      "loss: 1.928039  [128000/195814]\n",
      "loss: 2.930907  [160000/195814]\n",
      "loss: 4.149716  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.09, MAE: 0.23, Avg loss: 2.918396 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.370552  [    0/195814]\n",
      "loss: 2.441171  [32000/195814]\n",
      "loss: 1.531535  [64000/195814]\n",
      "loss: 2.764582  [96000/195814]\n",
      "loss: 1.700830  [128000/195814]\n",
      "loss: 4.117411  [160000/195814]\n",
      "loss: 3.320763  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.09, MAE: 0.22, Avg loss: 2.779284 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.663903  [    0/195814]\n",
      "loss: 1.698436  [32000/195814]\n",
      "loss: 1.876403  [64000/195814]\n",
      "loss: 2.745936  [96000/195814]\n",
      "loss: 2.100166  [128000/195814]\n",
      "loss: 2.416179  [160000/195814]\n",
      "loss: 2.778705  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.09, MAE: 0.22, Avg loss: 2.740913 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = BaseSalaryPredictor(\n",
    "    n_cat_features=len(preprocessor.categorical_vectorizer.vocabulary_),\n",
    "    n_tokens=len(preprocessor.vocab),\n",
    "    pad_ix=preprocessor.PAD_IX,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3369.892090  [    0/195814]\n",
      "loss: 9.517196  [32000/195814]\n",
      "loss: 4.546475  [64000/195814]\n",
      "loss: 5.803899  [96000/195814]\n",
      "loss: 2.532987  [128000/195814]\n",
      "loss: 4.635777  [160000/195814]\n",
      "loss: 2.603899  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.13, MAE: 0.28, Avg loss: 4.223629 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.150914  [    0/195814]\n",
      "loss: 4.661703  [32000/195814]\n",
      "loss: 2.658341  [64000/195814]\n",
      "loss: 1.875123  [96000/195814]\n",
      "loss: 2.306650  [128000/195814]\n",
      "loss: 1.467574  [160000/195814]\n",
      "loss: 1.763587  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.10, MAE: 0.24, Avg loss: 3.142331 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.113844  [    0/195814]\n",
      "loss: 1.348481  [32000/195814]\n",
      "loss: 2.215037  [64000/195814]\n",
      "loss: 2.731616  [96000/195814]\n",
      "loss: 2.714980  [128000/195814]\n",
      "loss: 1.680175  [160000/195814]\n",
      "loss: 2.388978  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.09, MAE: 0.22, Avg loss: 2.881999 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 5.132281  [    0/195814]\n",
      "loss: 2.599163  [32000/195814]\n",
      "loss: 1.285577  [64000/195814]\n",
      "loss: 2.219314  [96000/195814]\n",
      "loss: 2.388729  [128000/195814]\n",
      "loss: 1.032784  [160000/195814]\n",
      "loss: 2.203586  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.09, MAE: 0.22, Avg loss: 2.738905 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.818141  [    0/195814]\n",
      "loss: 1.853754  [32000/195814]\n",
      "loss: 4.483473  [64000/195814]\n",
      "loss: 1.805399  [96000/195814]\n",
      "loss: 1.510441  [128000/195814]\n",
      "loss: 1.891324  [160000/195814]\n",
      "loss: 1.390903  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.08, MAE: 0.22, Avg loss: 2.651065 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictor(\n",
    "    n_cat_features=len(preprocessor.categorical_vectorizer.vocabulary_),\n",
    "    n_tokens=len(preprocessor.vocab),\n",
    "    pad_ix=preprocessor.PAD_IX,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "`<YOUR_TEXT_HERE>`, i guess..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended options\n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm*`/`L.BatchNormalization`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to perform pooling:\n",
    "* Max over time (independently for each feature)\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$, where \n",
    "\n",
    "$$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a dense layer.\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n",
    "\n",
    "#### C) Fun with words\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not download pre-trained embeddings from [here](http://nlp.stanford.edu/data/glove.6B.zip) and follow this [manual](https://keras.io/examples/nlp/pretrained_word_embeddings/) to initialize your Keras embedding layer with downloaded weights.\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [early stopping callback(keras)](https://keras.io/callbacks/#earlystopping) or in [pytorch(lightning)](https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html).\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.save(file_name)`\n",
    "  * Plotting learning curves is usually a good idea\n",
    "  \n",
    "Good luck! And may the force be with you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why did we use `max_len = 10` for `FullDescription` column?  \n",
    "The text size in this column is so much bigger\n",
    "* Why do we treat `Company` and `Title` differently?\n",
    "* Investigate lengths for `Title` and `FullDescription` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yandex_nlp",
   "language": "python",
   "name": "yandex_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
