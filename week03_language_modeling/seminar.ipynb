{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram language models or how to write scientific papers (4 pts)\n",
    "\n",
    "We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n",
    "\n",
    "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
    "\n",
    "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n",
    "\n",
    "_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x arxivData.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34395</th>\n",
       "      <td>[{'name': 'Richard Moot'}]</td>\n",
       "      <td>6</td>\n",
       "      <td>1606.01720v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>6</td>\n",
       "      <td>We present a proof net calculus for the Displa...</td>\n",
       "      <td>[{'term': 'cs.LO', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Proof nets for the Displacement calculus</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12568</th>\n",
       "      <td>[{'name': 'Fanhua Shang'}, {'name': 'Yuanyuan ...</td>\n",
       "      <td>28</td>\n",
       "      <td>1803.00420v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>2</td>\n",
       "      <td>The Schatten quasi-norm was introduced to brid...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Tractable and Scalable Schatten Quasi-Norm App...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6180</th>\n",
       "      <td>[{'name': 'Kwang-Sung Jun'}, {'name': 'Robert ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1609.00845v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>9</td>\n",
       "      <td>In graph-based active learning, algorithms bas...</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
       "      <td>Graph-Based Active Learning: A New Look at Exp...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7507</th>\n",
       "      <td>[{'name': 'Sarah Tan'}, {'name': 'Rich Caruana...</td>\n",
       "      <td>26</td>\n",
       "      <td>1801.08640v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>1</td>\n",
       "      <td>Model distillation was originally designed to ...</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
       "      <td>Transparent Model Distillation</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27356</th>\n",
       "      <td>[{'name': 'Lei Bi'}, {'name': 'Jinman Kim'}, {...</td>\n",
       "      <td>12</td>\n",
       "      <td>1703.04197v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>3</td>\n",
       "      <td>Malignant melanoma has one of the most rapidly...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Automatic Skin Lesion Analysis using Large-sca...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  author  day            id  \\\n",
       "34395                         [{'name': 'Richard Moot'}]    6  1606.01720v1   \n",
       "12568  [{'name': 'Fanhua Shang'}, {'name': 'Yuanyuan ...   28  1803.00420v1   \n",
       "6180   [{'name': 'Kwang-Sung Jun'}, {'name': 'Robert ...    3  1609.00845v1   \n",
       "7507   [{'name': 'Sarah Tan'}, {'name': 'Rich Caruana...   26  1801.08640v1   \n",
       "27356  [{'name': 'Lei Bi'}, {'name': 'Jinman Kim'}, {...   12  1703.04197v2   \n",
       "\n",
       "                                                    link  month  \\\n",
       "34395  [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
       "12568  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
       "6180   [{'rel': 'alternate', 'href': 'http://arxiv.or...      9   \n",
       "7507   [{'rel': 'alternate', 'href': 'http://arxiv.or...      1   \n",
       "27356  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
       "\n",
       "                                                 summary  \\\n",
       "34395  We present a proof net calculus for the Displa...   \n",
       "12568  The Schatten quasi-norm was introduced to brid...   \n",
       "6180   In graph-based active learning, algorithms bas...   \n",
       "7507   Model distillation was originally designed to ...   \n",
       "27356  Malignant melanoma has one of the most rapidly...   \n",
       "\n",
       "                                                     tag  \\\n",
       "34395  [{'term': 'cs.LO', 'scheme': 'http://arxiv.org...   \n",
       "12568  [{'term': 'cs.LG', 'scheme': 'http://arxiv.org...   \n",
       "6180   [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
       "7507   [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
       "27356  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "\n",
       "                                                   title  year  \n",
       "34395           Proof nets for the Displacement calculus  2016  \n",
       "12568  Tractable and Scalable Schatten Quasi-Norm App...  2018  \n",
       "6180   Graph-Based Active Learning: A New Look at Exp...  2016  \n",
       "7507                      Transparent Model Distillation  2018  \n",
       "27356  Automatic Skin Lesion Analysis using Large-sca...  2017  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
    "# !wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
    "!tar -xvzf arxivData.json.tar.gz\n",
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41000, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble lines: concatenate title and description\n",
    "lines = data.apply(lambda row: row[\"title\"] + \" ; \" + row[\"summary\"], axis=1).tolist()\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26157fc214be42088ab4678bb3615571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenization:   0%|          | 0/41000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Task: convert lines (in-place) into strings of space-separated tokens. import & use WordPunctTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "lines = [\n",
    "    \" \".join(tokenizer.tokenize(line.lower()))\n",
    "    for line in tqdm(lines, desc=\"Tokenization\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    sorted(lines, key=len)[0]\n",
    "    == \"differential contrastive divergence ; this paper has been retracted .\"\n",
    ")\n",
    "assert (\n",
    "    sorted(lines, key=len)[2]\n",
    "    == \"p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Language Model (1 point)\n",
    "\n",
    "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
    "\n",
    "It can do so by following the chain rule:\n",
    "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
    "\n",
    "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
    "\n",
    "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
    "\n",
    "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
    "\n",
    "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words. \n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
    "$$\n",
    "\n",
    "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first stage to building such a model is counting all word occurrences given N-1 previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens:\n",
    "# - unk represents absent tokens,\n",
    "# - eos is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "\n",
    "def count_ngrams(lines, n):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary {tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "\n",
    "    for line in tqdm(lines, desc=\"Counting n-grams\"):\n",
    "        unk_prefix = \" \".join([UNK] * (n - 1)) if n > 1 else UNK\n",
    "        eos_suffix = f\"{EOS}\"\n",
    "        tokens = (unk_prefix + \" \" + line + \" \" + eos_suffix).split()\n",
    "\n",
    "        for i in range(n - 1, len(tokens)):\n",
    "            n_gram = tuple(tokens[i - n + 1: i])\n",
    "            counts[n_gram].update([tokens[i]])\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf32b9436aa4c70accd3b4886d7be4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's test it\n",
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
    "assert len(dummy_counts[(\"_UNK_\", \"_UNK_\")]) == 78\n",
    "assert dummy_counts[\"_UNK_\", \"a\"][\"note\"] == 3\n",
    "assert dummy_counts[\"p\", \"=\"][\"np\"] == 2\n",
    "assert dummy_counts[\"author\", \".\"][\"_EOS_\"] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'has': 14,\n",
       "         'specifies': 1,\n",
       "         'presents': 3,\n",
       "         ',': 1,\n",
       "         'we': 3,\n",
       "         'describes': 1})"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_counts[\"this\", \"paper\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we can count N-grams, we can build a probabilistic language model.  \n",
    "The simplest way to compute probabilities is in proportion to counts:\n",
    "\n",
    "$$ \\normalsize P(w_t | \\text{prefix}) = \\frac{\\text{Count}(\\text{prefix}, w_t)}{\\sum_{\\hat w} \\text{Count}(\\text{prefix}, \\hat w)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\"\n",
    "        Train a simple count-based language model:\n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "\n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "\n",
    "        # compute token proabilities given counts\n",
    "        self.probs = defaultdict(Counter)\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "\n",
    "        # populate self.probs with actual probabilities\n",
    "        for key, values in tqdm(\n",
    "            counts.items(), desc=\"Calculating probabilities given counts\"\n",
    "        ):\n",
    "            s = sum(values.values())\n",
    "            for word, count in values.items():\n",
    "                self.probs[key][word] = counts[key][word] / s\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix: str) -> dict:\n",
    "        \"\"\"\n",
    "        Getting dict of next tokens with corresponding probabilities to occur after the `prefix`\n",
    "\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "\n",
    "        Examples with n = 3:\n",
    "        Input prefix: \"\"\n",
    "        Preprocessed prefix (\"_UNK_\", \"_UNK_\")\n",
    "        Result: dict of tokens with corresponding probabilities to occur at the beginning of the sentence\n",
    "\n",
    "        Input prefix: \"the\"\n",
    "        Preprocessed prefix (\"_UNK_\", \"the\")\n",
    "        Result: dict of tokens with corresponding probabilities to occur after the word \"The\"\n",
    "\n",
    "        Input prefix: \"This paper\"\n",
    "        Preprocessed prefix (\"this\", \"paper\")\n",
    "        Result: dict of tokens with corresponding probabilities to occur after the phrase \"this paper\"\n",
    "\n",
    "        Input prefix: \"This paper is about\"\n",
    "        Preprocessed prefix (\"is\", \"about\")\n",
    "        Result: dict of tokens with corresponding probabilities to occur after the phrase \"is about\".\n",
    "        \n",
    "        Notice that the prefix is clipped and only the last n - 1 words were taken.\n",
    "        \"\"\"\n",
    "\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1) :]\n",
    "        prefix = [UNK] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "\n",
    "    def get_next_token_prob(self, prefix: str, next_token: str) -> float:\n",
    "        \"\"\"\n",
    "        Getting the probability of seeing \"next_token\" after the \"prefix\"\n",
    "\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c058ea6cccdb40f7a49b6326dc477cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034d76b421ea4846a5e3882a79626188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/2086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens(\"\")  # '' -> ['_UNK_', '_UNK_']\n",
    "assert np.allclose(p_initial[\"learning\"], 0.02)\n",
    "assert np.allclose(p_initial[\"a\"], 0.13)\n",
    "assert np.allclose(p_initial.get(\"meow\", 0), 0)\n",
    "assert np.allclose(sum(p_initial.values()), 1)\n",
    "\n",
    "p_a = dummy_lm.get_possible_next_tokens(\"a\")  # 'a' -> ['_UNK_', 'a']\n",
    "assert np.allclose(p_a[\"machine\"], 0.15384615)\n",
    "assert np.allclose(p_a[\"note\"], 0.23076923)\n",
    "assert np.allclose(p_a.get(\"the\", 0), 0)\n",
    "assert np.allclose(sum(p_a.values()), 1)\n",
    "\n",
    "assert np.allclose(dummy_lm.get_possible_next_tokens(\"a note\")[\"on\"], 1)\n",
    "assert dummy_lm.get_possible_next_tokens(\n",
    "    \"a machine\"\n",
    ") == dummy_lm.get_possible_next_tokens(\n",
    "    \"there have always been ghosts in a machine\"\n",
    "), \"your 3-gram model should only depend on 2 previous words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61617557a434d8cb107575db7e77ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/41000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b833550cf364064b1bd1e3aeb1b72a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/1219478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm = NGramLanguageModel(lines, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
    "\n",
    "$ X = [] $\n",
    "\n",
    "__forever:__\n",
    "* $w_{next} \\sim P(w_{next} | X)$\n",
    "* $X = concat(X, w_{next})$\n",
    "\n",
    "\n",
    "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature.  \n",
    "In the latter case (temperature), one samples from\n",
    "\n",
    "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
    "\n",
    "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(lm, prefix, temperature=1.0):\n",
    "    \"\"\"\n",
    "    return next token after prefix;\n",
    "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "    next_tokens: dict = lm.get_possible_next_tokens(prefix)\n",
    "\n",
    "    if temperature == 0:\n",
    "        sorted_next_tokens = dict(\n",
    "            sorted(tuple(next_tokens.items()), key=lambda item: item[1], reverse=True)\n",
    "        )\n",
    "        next_token = tuple(sorted_next_tokens.items())[0][0]\n",
    "    else:\n",
    "        sum_probs = sum([prob ** (1 / temperature) for prob in next_tokens.values()])\n",
    "\n",
    "        next_tokens = {\n",
    "            token: prob ** (1 / temperature) / sum_probs\n",
    "            for token, prob in next_tokens.items()\n",
    "        }\n",
    "\n",
    "        tokens = list(next_tokens.keys())\n",
    "        probs = list(next_tokens.values())\n",
    "        next_token = np.random.choice(tokens, 1, p=probs)[0]\n",
    "\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks nice!\n",
      "CPU times: total: 4.34 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, \"there have\") for _ in range(10000)])\n",
    "assert 250 < test_freqs[\"not\"] < 450\n",
    "assert 8500 < test_freqs[\"been\"] < 9500\n",
    "assert 1 < test_freqs[\"lately\"] < 200\n",
    "\n",
    "test_freqs = Counter(\n",
    "    [get_next_token(lm, \"deep\", temperature=1.0) for _ in range(10000)]\n",
    ")\n",
    "assert 1500 < test_freqs[\"learning\"] < 3000\n",
    "test_freqs = Counter(\n",
    "    [get_next_token(lm, \"deep\", temperature=0.5) for _ in range(10000)]\n",
    ")\n",
    "assert 8000 < test_freqs[\"learning\"] < 9000\n",
    "test_freqs = Counter(\n",
    "    [get_next_token(lm, \"deep\", temperature=0.0) for _ in range(10000)]\n",
    ")\n",
    "assert test_freqs[\"learning\"] == 10000\n",
    "\n",
    "print(\"Looks nice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have fun with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural data to obtain an appropriate dataset can be trained to generate poisoned data : discriminative embeddings . we first look at , we follow a statistical basis the higher levels of happiness . we design a physics simulation ; simulation - based or watershed - based reasoning system can support visual classification , logistic regression in high dimensional learning problems . moreover , very few training examples convey information to build novel models , because outputs are the two approaches to image noise , which for fully automatic method employing a hierarchical deep learning algorithms which are optimized to preserve\n"
     ]
    }
   ],
   "source": [
    "prefix = \"natural\"  # <- your ideas :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += \" \" + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing tasks . we also show that the proposed method outperforms existing state - of - the - art methods . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = \"natural language\"  # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += \" \" + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__More in the homework:__ nucleus sampling, top-k sampling, beam search (not for the faint of heart)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating language models: perplexity (1 point)\n",
    "\n",
    "Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n",
    "\n",
    "To compute perplexity on one sentence, use:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "\n",
    "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of 1, divided by __total length of all sentences__ in corpora.\n",
    "\n",
    "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(lm, lines, min_logprob=np.log(10**-50.0)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "\n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "\n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "\n",
    "    total_length = 0\n",
    "    log_pp = 0\n",
    "\n",
    "    for line in tqdm(lines, desc=\"Calculating perplexity for corpora\"):\n",
    "        tokens = [\"\"] + line.split(\" \") + [EOS]\n",
    "\n",
    "        for t in range(1, len(tokens)):\n",
    "            prefix = \" \".join(tokens[:t])\n",
    "            log_pp += max(\n",
    "                min_logprob, np.log(lm.get_next_token_prob(prefix, tokens[t]))\n",
    "            )\n",
    "            total_length += 1\n",
    "\n",
    "    return np.exp(-(1 / total_length) * log_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbe14817653453bb564b46693a5e8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f98aaaa48014692a460f7214c267a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a10d13c32431aaf575de707cdc403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87eafcf09e8145098d4cda2f50c6f29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/2086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeee2c777594d00ba472e1ae808d71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16b07a4cecf4260900044ed37cdb195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/2703 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a251ef24b2bd4092b3c872bb5ca929e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating perplexity for corpora:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df55c03abe474693ae7e692c75ca5133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating perplexity for corpora:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903f0ed9e1fe4ab09b7daaf1effffbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating perplexity for corpora:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbb392629cd44a99fa9444421b000b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating perplexity for corpora:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=329.27383 ppx3=1.52000 ppx10=1.18381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryblo\\AppData\\Local\\Temp\\ipykernel_7784\\1882133004.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "  min_logprob, np.log(lm.get_next_token_prob(prefix, tokens[t]))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [580]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ppx1 \u001b[38;5;241m>\u001b[39m ppx3 \u001b[38;5;241m>\u001b[39m ppx10, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigher N models should overfit and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(ppx_missing) \u001b[38;5;129;01mand\u001b[39;00m ppx_missing \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m, (\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing words should have large but finite perplexity. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure you use min_logprob right\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[0;32m     24\u001b[0m     [ppx1, ppx3, ppx10], [\u001b[38;5;241m318.2132342216302\u001b[39m, \u001b[38;5;241m1.5199996213739575\u001b[39m, \u001b[38;5;241m1.1838145037901249\u001b[39m]\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "\n",
    "ppx_missing = perplexity(\n",
    "    lm3, [\"the jabberwock , with eyes of flame , \"]\n",
    ")  # thanks, L. Carrol\n",
    "\n",
    "print(\"Perplexities: ppx1=%.5f ppx3=%.5f ppx10=%.5f\" % (ppx1, ppx3, ppx10))\n",
    "\n",
    "assert all(\n",
    "    0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)\n",
    "), \"perplexity should be nonnegative and reasonably small\"\n",
    "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
    "assert np.isfinite(ppx_missing) and ppx_missing > 10**6, (\n",
    "    \"missing words should have large but finite perplexity. \"\n",
    "    \" Make sure you use min_logprob right\"\n",
    ")\n",
    "assert np.allclose(\n",
    "    [ppx1, ppx3, ppx10], [318.2132342216302, 1.5199996213739575, 1.1838145037901249]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639bdbe1e63f4512abb45ed502b62a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/30750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3f19f632a24eefa8e95c092e95fb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c139eafe2bac4abf9c9b0dc4aa393b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating perplexity for corpora:   0%|          | 0/10250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryblo\\AppData\\Local\\Temp\\ipykernel_7784\\1882133004.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "  min_logprob, np.log(lm.get_next_token_prob(prefix, tokens[t]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1841.86161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0239279474d4c3e8ccda6d2859e106c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/30750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38c86007df443b9bbcdd6b6eddbb9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/54176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730e741617124b6eb866cff7856ec0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating perplexity for corpora:   0%|          | 0/10250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 85653987.28774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e82a451629445fb8f65a7eeb3aee6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting n-grams:   0%|          | 0/30750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0b1ef4908945fb89305b431e1841ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating probabilities given counts:   0%|          | 0/1005464 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe79732a23c842b79112f53b87fddce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating perplexity for corpora:   0%|          | 0/10250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 61999196259042902147072.00000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoops, it just blew up :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM Smoothing\n",
    "\n",
    "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
    "\n",
    "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
    "\n",
    "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
    "\n",
    "Here's an example code we've implemented for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel):\n",
    "    \"\"\"this code is an example, no need to change anything\"\"\"\n",
    "\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(\n",
    "            token for token_counts in counts.values() for token in token_counts\n",
    "        )\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {\n",
    "                token: (token_counts[token] + delta) / total_count\n",
    "                for token in token_counts\n",
    "            }\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "\n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total)  # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(\n",
    "        sum([dummy_lm.get_next_token_prob(\"a\", w_i) for w_i in dummy_lm.vocab]), 1\n",
    "    ), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: try to sample tokens from such a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kneser-Ney smoothing (2 points)\n",
    "\n",
    "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
    "\n",
    "\n",
    "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
    "\n",
    "It can be computed recurrently, for n>1:\n",
    "\n",
    "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
    "\n",
    "where\n",
    "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
    "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
    "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
    "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
    "\n",
    "See lecture slides or wiki for more detailed formulae.\n",
    "\n",
    "__Your task__ is to\n",
    "- implement KneserNeyLanguageModel\n",
    "- test it on 1-3 gram language models\n",
    "- find optimal (within reasoning) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneserNeyLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        <YOUR CODE>\n",
    "        \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        < YOUR CODE >\n",
    "        \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(\n",
    "        sum([dummy_lm.get_next_token_prob(\"a\", w_i) for w_i in dummy_lm.vocab]), 1\n",
    "    ), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = KneserNeyLanguageModel(train_lines, n=n, smoothing=<...>)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yandex_nlp",
   "language": "python",
   "name": "yandex_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
